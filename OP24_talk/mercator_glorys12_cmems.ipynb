{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODXMzeJJ4qkvwlBg6l0Kml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanAkel/ocean-hurricane/blob/main/mercator_glorys12_cmems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To download data from [Mercator Glorys 1/12-deg reanalysis](https://data.marine.copernicus.eu/product/GLOBAL_ANALYSISFORECAST_PHY_001_024/description)\n",
        "\n"
      ],
      "metadata": {
        "id": "YBObW_mhg7-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User inputs, should be consistent with those in:\n",
        "# https://github.com/sanAkel/ocean-hurricane/blob/main/get_track.ipynb\n",
        "\n",
        "# Basin, year and category\n",
        "myBasin = 'north_atlantic'\n",
        "year = 2024\n",
        "cat_threshold = 4\n",
        "\n",
        "# Buffer (time and space)\n",
        "day_buffer = 2 # extract CMEMS data hurricane start to end date plus/minus buffer\n",
        "lat_buffer, lon_buffer = [10, 10] # extent of data to extract with respect to storm track latitude/longitude extent"
      ],
      "metadata": {
        "id": "ijFCEEy_hahJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install API"
      ],
      "metadata": {
        "id": "UzD_EfkUhdyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install copernicusmarine"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OVhx98lig84x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copernicusmarine\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "dTEeME9mhIRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up environment for the [Copernicus Marine Toolbox.](https://help.marine.copernicus.eu/en/articles/7949409-copernicus-marine-toolbox-introduction)"
      ],
      "metadata": {
        "id": "QnscMBpUh31f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(copernicusmarine.__version__)\n",
        "copernicusmarine.login(username=\"sakella\", password=\"HbFPyP9M\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vd7leka8h_4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copernicus marine dataset\n",
        "# https://catalogue.marine.copernicus.eu/documents/PUM/CMEMS-GLO-PUM-001-024.pdf\n",
        "# ----\n",
        "# Which ocean dataset to read?\n",
        "# Hourly mean surface (2d) fields: cmems_mod_glo_phy_anfc_0.083deg_PT1H-m\n",
        "# Instantaneous (inst) 6-hourly 3d potential temperature: cmems_mod_glo_phy-thetao_anfc_0.083deg_PT6H-i\n",
        "# inst 6hr 3d salinity: cmems_mod_glo_phy-so_anfc_0.083deg_PT6H-i\n",
        "# inst 6hr 3d currents: cmems_mod_glo_phy-cur_anfc_0.083deg_PT6H-i\n",
        "# ----\n",
        "\n",
        "# Set parameters\n",
        "data_request = {\n",
        "    \"dataset_id\" : \"cmems_mod_glo_phy_anfc_0.083deg_PT1H-m\",\n",
        "    \"longitude\" : [-180, 180],\n",
        "    \"latitude\" : [-80, 90],\n",
        "    \"variables\" : [\"thetao\", \"so\", \"uo\", \"vo\", \"zos\"] # changes based on dataset_id\n",
        "}"
      ],
      "metadata": {
        "id": "4eRjASB3iDYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cms_data =copernicusmarine.open_dataset(\n",
        "    dataset_id = data_request[\"dataset_id\"],\n",
        "    minimum_longitude = data_request[\"longitude\"][0],\n",
        "    maximum_longitude = data_request[\"longitude\"][1],\n",
        "    minimum_latitude = data_request[\"latitude\"][0],\n",
        "    maximum_latitude = data_request[\"latitude\"][1],\n",
        "    variables = data_request[\"variables\"]\n",
        ")"
      ],
      "metadata": {
        "id": "h2ppCFJ-omRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFirst available date for this dataset:\\t{}\".format(cms_data.isel(time=0).time.values.astype(str).split('T')[0]))"
      ],
      "metadata": {
        "id": "wikIQ_r0pQnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the processed/downloaded hurricane data. It was generated using [this notebook.](https://github.com/sanAkel/ocean-hurricane/blob/main/get_track.ipynb)"
      ],
      "metadata": {
        "id": "iJIb87gUrf_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive - to save files once done\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wmyTqhJ0rpec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read downloaded track data, subset and save CMEMS data for the hurricane dates\n",
        "input_data_path = '/content/drive/MyDrive/datasets/hurr/{}/'.format(year)\n",
        "input_fName = input_data_path + 'hurdat2_{}_{}.csv'.format(myBasin, year)\n",
        "print(\"Reading {} summary data from:\\n{}\".format(year, input_fName))\n",
        "\n",
        "season_data=pd.read_csv(input_fName)\n",
        "major_hurr_names = season_data['name'][season_data['category'] >=cat_threshold]\n",
        "major_hurr_ids = season_data['id'][season_data['category'] >=cat_threshold]\n",
        "\n",
        "print(\"\\n\\nStorms that had a category >= {} are following:\\n\".format(cat_threshold))\n",
        "for hurr in major_hurr_names:\n",
        "    print(hurr)"
      ],
      "metadata": {
        "id": "PhAP3tZ3rzHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read storm data and \"download\" data\n",
        "for idx, hurr_id in enumerate(major_hurr_ids):\n",
        "    print(\"{}, ID: [{}]\".format(major_hurr_names.iloc[idx], hurr_id))\n",
        "    input_hurr_file = str(year) + \"_\" + myBasin + \"_\" + major_hurr_names.iloc[idx] + '.nc'\n",
        "    print(\"Processed file name:\\t{}\".format(input_data_path + input_hurr_file))\n",
        "    track_ds = xr.open_dataset(input_data_path + input_hurr_file)\n",
        "    print(\"Start and end days:\\n{}--\\t{}.\\n\".format(track_ds.time[0].values, track_ds.time[-1].values))\n",
        "\n",
        "    ts, te = [track_ds.time[0].values- np.timedelta64(day_buffer, 'D'), track_ds.time[-1].values + np.timedelta64(day_buffer, 'D')]\n",
        "    lat_s, lat_e = [track_ds.lat[0].values-lat_buffer, track_ds.lat[-1].values+lat_buffer]\n",
        "    lon_s, lon_e = [track_ds.lon[-1].values-lon_buffer, track_ds.lon[0].values+lon_buffer] # Western hemisphere negative lon\n",
        "    hurr_subset=cms_data.sel(time=slice(ts, te), latitude=slice(lat_s, lat_e), longitude=slice(lon_s, lon_e))\n",
        "\n",
        "    output_hurr_file = str(year) + \"_\" + myBasin + \"_\" + major_hurr_names.iloc[idx] + '_GLORYS12.nc'\n",
        "    print(\"Output file name:\\t{}\\n\\n\".format(input_data_path + output_hurr_file))\n",
        "    hurr_subset.to_netcdf(input_data_path + output_hurr_file)"
      ],
      "metadata": {
        "id": "YQPkCRgVsKPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}